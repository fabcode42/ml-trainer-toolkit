

# ğŸš€ Prompt Engineering Examples for Generative AI on AWS Bedrock

**Live Demos using: Titan, Claude 3, and DeepSeek**  
ğŸ“ _Location: In-person workshop in Prague_  
ğŸ¯ _Audience: Primarily developers, possibly mixed (business & tech)_

This guide includes real-world prompt engineering examples to demonstrate the capabilities and differences between various foundation models available in AWS Bedrock. These prompts are designed to be showcased live using the Bedrock Playground.

---

## 1. âœï¸ Summarization & Rephrasing (Business Tone)

```text
Summarize the following technical document into a concise executive summary suitable for senior management. Maintain a professional and clear tone.

---
[Paste any technical AWS architecture or whitepaper excerpt, e.g., 200â€“300 words]

ğŸ§  Purpose: Highlights how Claude 3 handles tone and clarity. Titan offers brevity. DeepSeek often provides structured summaries.
2. ğŸ’¡ Creative Ideation (Startup Focus)

Generate a unique startup idea in the sustainability space. Include:
- A name
- A one-sentence value proposition
- A brief description of the target market
- A monetization strategy

Be original but realistic, suitable for a European audience.

ğŸ§  Purpose: Great for demonstrating model creativity and structure. Claude excels in concept richness, DeepSeek in clarity.
3. ğŸ’» Coding Task: Python Function + Unit Test

Write a Python function that filters out all non-prime numbers from a list of integers.
Then write a set of unit tests using `unittest` to verify the function works correctly.
Use clean, readable code and include comments where appropriate.

ğŸ§  Purpose: DeepSeek generally excels here. Titan performs well with direct tasks. Claude explains logic nicely.
4. ğŸ¯ Prompt Optimization Demo (Before vs. After)
â– Basic Prompt

Write a product description.

â• Optimized Prompt

Write a 150-word marketing description for a smart home energy-saving device.
Use an informal yet professional tone, target 25â€“40-year-old urban professionals in Europe,
and focus on environmental and cost-saving benefits.

ğŸ§  Purpose: Perfect for showing the importance of specificity. All models benefit from prompt refinement.
5. ğŸ§  Few-Shot Classification Task (Supervised Prompt)

Classify the following customer feedback into one of these categories: `Positive`, `Negative`, or `Neutral`.
Examples:
- "I love how fast the app is!" â†’ Positive
- "The UI is confusing and hard to use." â†’ Negative
- "It works as expected." â†’ Neutral

Now classify this feedback:
"The latest update improved speed, but there are still occasional crashes."

ğŸ§  Purpose: Shows how models apply patterns from few-shot examples. Claude is particularly strong here.
6. ğŸ—‚ï¸ Structured Output Formatting

Extract structured information from the following paragraph and output it in JSON format.
Include fields: `Name`, `Role`, `Company`, `Key Achievement`.

Text:
"Anna Kowalska, a senior data scientist at Novagen in Prague, led the development of a new fraud detection model that reduced false positives by 45%."

ğŸ§  Purpose: Ideal to compare JSON formatting and accuracy across models.
ğŸ§ª Bonus: Prompt Pitfalls
Vague Prompt

Explain cloud security.

Improved Prompt

Explain three common AWS cloud security threats and how to mitigate each,
using simple language suitable for business stakeholders.

ğŸ§  Purpose: Teaches how vague prompts lead to vague outputs. Demonstrates the power of good scoping.
âœ… How to Use These Prompts in Class

    Demo each prompt live in Bedrock Playground with Titan, Claude 3, and DeepSeek.

    Ask the audience:

        Which result is most useful?

        How could the prompt be improved?

    Invite iterations: Try student suggestions live.

    Discuss real-world use cases: Chatbots, automation, summarization, dev support, etc.

ğŸ“ License

Feel free to reuse or remix these prompts in your own workshops. Attribution welcome, but not required.






